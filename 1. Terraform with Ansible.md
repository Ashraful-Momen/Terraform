# Complete Terraform + Ansible Integration Guide

## 🎯 Overview: Why Terraform + Ansible?

**Terraform**: Infrastructure as Code (IaC) - Provisions infrastructure
**Ansible**: Configuration Management - Configures and manages infrastructure

### Perfect Division of Responsibilities
- **Terraform**: Creates VMs, networks, security groups, load balancers
- **Ansible**: Installs software, configures services, deploys applications

## 🚀 Setup & Prerequisites

### Install Required Tools
```bash
# Terraform
curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -
sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"
sudo apt-get update && sudo apt-get install terraform

# Ansible (if not already installed)
sudo apt update && sudo apt install ansible

# AWS CLI (for AWS provider)
sudo apt install awscli
aws configure
```

### Verify Installation
```bash
terraform version
ansible --version
aws --version
```

## 📁 Project Structure (Recommended)

```
project/
├── terraform/
│   ├── main.tf              # Main infrastructure
│   ├── variables.tf         # Input variables
│   ├── outputs.tf           # Output values
│   ├── providers.tf         # Provider configurations
│   └── terraform.tfvars     # Variable values
├── ansible/
│   ├── inventory/
│   │   ├── hosts           # Static inventory
│   │   └── group_vars/     # Group variables
│   ├── playbooks/          # Ansible playbooks
│   ├── roles/              # Custom roles
│   └── ansible.cfg         # Ansible config
├── scripts/
│   └── deploy.sh           # Deployment script
└── README.md
```

## 🏗️ Terraform Configuration

### providers.tf
```hcl
terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    local = {
      source  = "hashicorp/local"
      version = "~> 2.1"
    }
  }
}

provider "aws" {
  region = var.aws_region
}
```

### variables.tf
```hcl
variable "aws_region" {
  description = "AWS region"
  type        = string
  default     = "us-west-2"
}

variable "environment" {
  description = "Environment name"
  type        = string
  default     = "dev"
}

variable "instance_type" {
  description = "EC2 instance type"
  type        = string
  default     = "t3.micro"
}

variable "key_name" {
  description = "AWS key pair name"
  type        = string
}

variable "web_server_count" {
  description = "Number of web servers"
  type        = number
  default     = 2
}

variable "allowed_cidr_blocks" {
  description = "CIDR blocks allowed to access instances"
  type        = list(string)
  default     = ["0.0.0.0/0"]
}
```

### main.tf
```hcl
# Data sources
data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"] # Canonical

  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-22.04-amd64-server-*"]
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }
}

data "aws_availability_zones" "available" {
  state = "available"
}

# VPC
resource "aws_vpc" "main" {
  cidr_block           = "10.0.0.0/16"
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name        = "${var.environment}-vpc"
    Environment = var.environment
  }
}

# Internet Gateway
resource "aws_internet_gateway" "main" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name        = "${var.environment}-igw"
    Environment = var.environment
  }
}

# Subnet
resource "aws_subnet" "public" {
  count                   = 2
  vpc_id                  = aws_vpc.main.id
  cidr_block              = "10.0.${count.index + 1}.0/24"
  availability_zone       = data.aws_availability_zones.available.names[count.index]
  map_public_ip_on_launch = true

  tags = {
    Name        = "${var.environment}-public-subnet-${count.index + 1}"
    Environment = var.environment
  }
}

# Route Table
resource "aws_route_table" "public" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name        = "${var.environment}-public-rt"
    Environment = var.environment
  }
}

# Route Table Association
resource "aws_route_table_association" "public" {
  count          = length(aws_subnet.public)
  subnet_id      = aws_subnet.public[count.index].id
  route_table_id = aws_route_table.public.id
}

# Security Group
resource "aws_security_group" "web" {
  name        = "${var.environment}-web-sg"
  description = "Security group for web servers"
  vpc_id      = aws_vpc.main.id

  ingress {
    description = "SSH"
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = var.allowed_cidr_blocks
  }

  ingress {
    description = "HTTP"
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    description = "HTTPS"
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name        = "${var.environment}-web-sg"
    Environment = var.environment
  }
}

# Load Balancer
resource "aws_lb" "web" {
  name               = "${var.environment}-web-lb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.web.id]
  subnets            = aws_subnet.public[*].id

  enable_deletion_protection = false

  tags = {
    Name        = "${var.environment}-web-lb"
    Environment = var.environment
  }
}

# Target Group
resource "aws_lb_target_group" "web" {
  name     = "${var.environment}-web-tg"
  port     = 80
  protocol = "HTTP"
  vpc_id   = aws_vpc.main.id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher             = "200"
    path                = "/"
    port                = "traffic-port"
    protocol            = "HTTP"
    timeout             = 5
    unhealthy_threshold = 2
  }

  tags = {
    Name        = "${var.environment}-web-tg"
    Environment = var.environment
  }
}

# Load Balancer Listener
resource "aws_lb_listener" "web" {
  load_balancer_arn = aws_lb.web.arn
  port              = "80"
  protocol          = "HTTP"

  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.web.arn
  }
}

# EC2 Instances
resource "aws_instance" "web" {
  count                  = var.web_server_count
  ami                    = data.aws_ami.ubuntu.id
  instance_type          = var.instance_type
  key_name               = var.key_name
  vpc_security_group_ids = [aws_security_group.web.id]
  subnet_id              = aws_subnet.public[count.index % length(aws_subnet.public)].id

  user_data = <<-EOF
              #!/bin/bash
              apt-get update
              apt-get install -y python3 python3-pip
              pip3 install boto3
              EOF

  tags = {
    Name        = "${var.environment}-web-${count.index + 1}"
    Environment = var.environment
    Role        = "webserver"
  }
}

# Target Group Attachment
resource "aws_lb_target_group_attachment" "web" {
  count            = length(aws_instance.web)
  target_group_arn = aws_lb_target_group.web.arn
  target_id        = aws_instance.web[count.index].id
  port             = 80
}

# Generate Ansible Inventory
resource "local_file" "ansible_inventory" {
  content = templatefile("${path.module}/inventory.tpl", {
    web_servers = aws_instance.web[*]
    lb_dns      = aws_lb.web.dns_name
  })
  filename = "../ansible/inventory/hosts"

  depends_on = [aws_instance.web]
}

# Generate Ansible Variables
resource "local_file" "ansible_vars" {
  content = templatefile("${path.module}/group_vars.tpl", {
    environment    = var.environment
    lb_dns_name    = aws_lb.web.dns_name
    aws_region     = var.aws_region
  })
  filename = "../ansible/inventory/group_vars/all.yml"

  depends_on = [aws_lb.web]
}
```

### outputs.tf
```hcl
output "vpc_id" {
  description = "VPC ID"
  value       = aws_vpc.main.id
}

output "web_server_ips" {
  description = "Public IPs of web servers"
  value       = aws_instance.web[*].public_ip
}

output "load_balancer_dns" {
  description = "Load balancer DNS name"
  value       = aws_lb.web.dns_name
}

output "web_server_private_ips" {
  description = "Private IPs of web servers"
  value       = aws_instance.web[*].private_ip
}

output "ansible_inventory_file" {
  description = "Path to generated Ansible inventory"
  value       = local_file.ansible_inventory.filename
}
```

### inventory.tpl (Template for Ansible Inventory)
```ini
# Generated by Terraform
[webservers]
%{ for idx, server in web_servers ~}
web-${idx + 1} ansible_host=${server.public_ip} private_ip=${server.private_ip} instance_id=${server.id}
%{ endfor ~}

[webservers:vars]
ansible_user=ubuntu
ansible_ssh_private_key_file=~/.ssh/your-key.pem
ansible_ssh_common_args='-o StrictHostKeyChecking=no'

[loadbalancer]
lb ansible_host=${lb_dns} ansible_connection=local

[all:vars]
ansible_python_interpreter=/usr/bin/python3
```

### group_vars.tpl (Template for Ansible Variables)
```yaml
# Generated by Terraform
---
environment: ${environment}
aws_region: ${aws_region}
load_balancer_dns: ${lb_dns_name}

# Application variables
app_name: myapp
app_version: 1.0.0
app_port: 3000

# Nginx configuration
nginx_port: 80
nginx_ssl_port: 443

# Database configuration (example)
db_host: localhost
db_name: myapp_${environment}
db_user: myapp_user
```

### terraform.tfvars
```hcl
aws_region        = "us-west-2"
environment       = "dev"
instance_type     = "t3.micro"
key_name          = "your-key-pair"
web_server_count  = 2
allowed_cidr_blocks = ["0.0.0.0/0"]  # Restrict this in production
```

## 🔧 Ansible Configuration

### ansible/ansible.cfg
```ini
[defaults]
inventory = inventory/hosts
remote_user = ubuntu
host_key_checking = False
timeout = 30
gathering = smart
fact_caching = memory
retry_files_enabled = False

[ssh_connection]
ssh_args = -o ControlMaster=auto -o ControlPersist=60s
pipelining = True
```

### ansible/playbooks/site.yml (Main Playbook)
```yaml
---
- name: Configure Web Servers
  hosts: webservers
  become: yes
  roles:
    - common
    - nginx
    - application

- name: Configure Load Balancer Monitoring
  hosts: loadbalancer
  connection: local
  tasks:
    - name: Verify load balancer is accessible
      uri:
        url: "http://{{ load_balancer_dns }}"
        method: GET
      register: lb_check
      
    - name: Display load balancer status
      debug:
        msg: "Load balancer is {{ 'accessible' if lb_check.status == 200 else 'not accessible' }}"
```

### ansible/roles/common/tasks/main.yml
```yaml
---
- name: Update apt cache
  apt:
    update_cache: yes
    cache_valid_time: 3600

- name: Install common packages
  apt:
    name: "{{ item }}"
    state: present
  loop:
    - curl
    - wget
    - git
    - htop
    - unzip
    - python3-pip
    - awscli

- name: Create application user
  user:
    name: "{{ app_user | default('deploy') }}"
    shell: /bin/bash
    home: "/home/{{ app_user | default('deploy') }}"
    create_home: yes

- name: Configure timezone
  timezone:
    name: "{{ timezone | default('UTC') }}"

- name: Configure SSH security
  lineinfile:
    path: /etc/ssh/sshd_config
    line: "{{ item }}"
    create: yes
  loop:
    - "PermitRootLogin no"
    - "PasswordAuthentication no"
  notify: restart ssh

- name: Install security updates
  apt:
    upgrade: safe
```

### ansible/roles/nginx/tasks/main.yml
```yaml
---
- name: Install nginx
  apt:
    name: nginx
    state: present

- name: Remove default nginx site
  file:
    path: "{{ item }}"
    state: absent
  loop:
    - /etc/nginx/sites-enabled/default
    - /etc/nginx/sites-available/default
  notify: restart nginx

- name: Create nginx configuration
  template:
    src: nginx.conf.j2
    dest: /etc/nginx/sites-available/{{ app_name }}
  notify: restart nginx

- name: Enable nginx site
  file:
    src: /etc/nginx/sites-available/{{ app_name }}
    dest: /etc/nginx/sites-enabled/{{ app_name }}
    state: link
  notify: restart nginx

- name: Start and enable nginx
  service:
    name: nginx
    state: started
    enabled: yes

- name: Configure firewall for nginx
  ufw:
    rule: allow
    name: 'Nginx Full'
  when: ansible_os_family == "Debian"
```

### ansible/roles/nginx/templates/nginx.conf.j2
```nginx
server {
    listen {{ nginx_port }};
    server_name {{ ansible_default_ipv4.address }} {{ inventory_hostname }};

    location / {
        proxy_pass http://127.0.0.1:{{ app_port }};
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;
    }

    location /health {
        return 200 'healthy\n';
        add_header Content-Type text/plain;
    }

    # Logging
    access_log /var/log/nginx/{{ app_name }}_access.log;
    error_log /var/log/nginx/{{ app_name }}_error.log;
}
```

### ansible/roles/application/tasks/main.yml
```yaml
---
- name: Create application directory
  file:
    path: "/opt/{{ app_name }}"
    state: directory
    owner: "{{ app_user | default('deploy') }}"
    group: "{{ app_user | default('deploy') }}"
    mode: '0755'

- name: Install Node.js (example application)
  shell: |
    curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
    apt-get install -y nodejs
  args:
    creates: /usr/bin/node

- name: Create simple Node.js application
  copy:
    content: |
      const express = require('express');
      const app = express();
      const port = {{ app_port }};

      app.get('/', (req, res) => {
        res.json({
          message: 'Hello from {{ inventory_hostname }}!',
          environment: '{{ environment }}',
          timestamp: new Date().toISOString(),
          server: '{{ ansible_hostname }}'
        });
      });

      app.get('/health', (req, res) => {
        res.status(200).json({ status: 'healthy' });
      });

      app.listen(port, () => {
        console.log(`App running on port ${port}`);
      });
    dest: "/opt/{{ app_name }}/app.js"
    owner: "{{ app_user | default('deploy') }}"
    group: "{{ app_user | default('deploy') }}"

- name: Create package.json
  copy:
    content: |
      {
        "name": "{{ app_name }}",
        "version": "{{ app_version }}",
        "description": "Sample application",
        "main": "app.js",
        "dependencies": {
          "express": "^4.18.0"
        },
        "scripts": {
          "start": "node app.js"
        }
      }
    dest: "/opt/{{ app_name }}/package.json"
    owner: "{{ app_user | default('deploy') }}"
    group: "{{ app_user | default('deploy') }}"

- name: Install Node.js dependencies
  npm:
    path: "/opt/{{ app_name }}"
    state: present
  become_user: "{{ app_user | default('deploy') }}"

- name: Create systemd service file
  template:
    src: app.service.j2
    dest: "/etc/systemd/system/{{ app_name }}.service"
  notify:
    - reload systemd
    - restart application

- name: Start and enable application service
  service:
    name: "{{ app_name }}"
    state: started
    enabled: yes
```

### ansible/roles/application/templates/app.service.j2
```ini
[Unit]
Description={{ app_name | title }} Application
After=network.target

[Service]
Type=simple
User={{ app_user | default('deploy') }}
WorkingDirectory=/opt/{{ app_name }}
ExecStart=/usr/bin/node app.js
Restart=on-failure
Environment=NODE_ENV={{ environment }}
Environment=PORT={{ app_port }}

[Install]
WantedBy=multi-user.target
```

### ansible/roles/common/handlers/main.yml
```yaml
---
- name: restart ssh
  service:
    name: ssh
    state: restarted
```

### ansible/roles/nginx/handlers/main.yml
```yaml
---
- name: restart nginx
  service:
    name: nginx
    state: restarted
```

### ansible/roles/application/handlers/main.yml
```yaml
---
- name: reload systemd
  systemd:
    daemon_reload: yes

- name: restart application
  service:
    name: "{{ app_name }}"
    state: restarted
```

## 🚀 Deployment Scripts

### scripts/deploy.sh
```bash
#!/bin/bash

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Function to print colored output
print_status() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Check if required tools are installed
check_dependencies() {
    print_status "Checking dependencies..."
    
    for tool in terraform ansible aws; do
        if ! command -v $tool &> /dev/null; then
            print_error "$tool is not installed or not in PATH"
            exit 1
        fi
    done
    
    print_status "All dependencies are installed"
}

# Deploy infrastructure with Terraform
deploy_infrastructure() {
    print_status "Deploying infrastructure with Terraform..."
    
    cd terraform
    
    # Initialize Terraform
    print_status "Initializing Terraform..."
    terraform init
    
    # Plan deployment
    print_status "Planning Terraform deployment..."
    terraform plan
    
    # Apply changes
    print_status "Applying Terraform changes..."
    terraform apply -auto-approve
    
    cd ..
    
    print_status "Infrastructure deployment completed"
}

# Configure servers with Ansible
configure_servers() {
    print_status "Configuring servers with Ansible..."
    
    cd ansible
    
    # Wait for instances to be ready
    print_status "Waiting for instances to be ready..."
    sleep 60
    
    # Test connectivity
    print_status "Testing Ansible connectivity..."
    ansible all -m ping
    
    # Run playbook
    print_status "Running Ansible playbook..."
    ansible-playbook -i inventory/hosts playbooks/site.yml
    
    cd ..
    
    print_status "Server configuration completed"
}

# Verify deployment
verify_deployment() {
    print_status "Verifying deployment..."
    
    cd terraform
    LB_DNS=$(terraform output -raw load_balancer_dns)
    cd ..
    
    print_status "Load balancer DNS: $LB_DNS"
    
    # Wait a bit for services to start
    sleep 30
    
    # Test load balancer
    if curl -s "http://$LB_DNS" > /dev/null; then
        print_status "✅ Load balancer is responding"
        curl -s "http://$LB_DNS" | jq . || echo "Response received"
    else
        print_warning "⚠️  Load balancer is not responding yet"
    fi
}

# Cleanup function
cleanup() {
    print_status "Cleaning up resources..."
    
    cd terraform
    terraform destroy -auto-approve
    cd ..
    
    print_status "Cleanup completed"
}

# Main deployment function
main() {
    case "${1:-deploy}" in
        deploy)
            check_dependencies
            deploy_infrastructure
            configure_servers
            verify_deployment
            ;;
        destroy)
            cleanup
            ;;
        infra-only)
            check_dependencies
            deploy_infrastructure
            ;;
        config-only)
            configure_servers
            ;;
        verify)
            verify_deployment
            ;;
        *)
            echo "Usage: $0 {deploy|destroy|infra-only|config-only|verify}"
            echo ""
            echo "Commands:"
            echo "  deploy      - Full deployment (infrastructure + configuration)"
            echo "  destroy     - Destroy all resources"
            echo "  infra-only  - Deploy only infrastructure"
            echo "  config-only - Run only Ansible configuration"
            echo "  verify      - Verify deployment"
            exit 1
            ;;
    esac
}

main "$@"
```

### Make script executable
```bash
chmod +x scripts/deploy.sh
```

## 🔄 Advanced Integration Patterns

### 1. Using Terraform Provisioners
```hcl
resource "aws_instance" "web" {
  # ... other configuration ...

  # Run Ansible directly from Terraform
  provisioner "local-exec" {
    command = <<-EOT
      sleep 60
      cd ../ansible
      ansible-playbook -i inventory/hosts playbooks/site.yml --limit ${self.public_ip}
    EOT
  }
}
```

### 2. Dynamic Inventory with AWS Plugin
```yaml
# ansible/inventory/aws_ec2.yml
plugin: aws_ec2
regions:
  - us-west-2
filters:
  tag:Environment: dev
  instance-state-name: running
keyed_groups:
  - prefix: tag
    key: tags
  - prefix: instance_type
    key: instance_type
  - prefix: placement
    key: placement.region
hostnames:
  - ip-address
compose:
  ansible_host: public_ip_address
```

### Use dynamic inventory:
```bash
ansible-inventory -i inventory/aws_ec2.yml --graph
ansible-playbook -i inventory/aws_ec2.yml playbooks/site.yml
```

### 3. Terraform Remote State for Ansible
```hcl
# In Terraform outputs.tf
output "ansible_vars" {
  value = {
    vpc_id              = aws_vpc.main.id
    subnet_ids          = aws_subnet.public[*].id
    security_group_id   = aws_security_group.web.id
    load_balancer_dns   = aws_lb.web.dns_name
    instance_ips        = aws_instance.web[*].public_ip
  }
  sensitive = false
}
```

### Read Terraform state in Ansible:
```yaml
# In Ansible playbook
- name: Read Terraform outputs
  shell: cd ../terraform && terraform output -json
  register: tf_outputs
  delegate_to: localhost
  run_once: true

- name: Set facts from Terraform
  set_fact:
    tf_vars: "{{ tf_outputs.stdout | from_json }}"
  run_once: true

- name: Use Terraform outputs
  debug:
    msg: "VPC ID: {{ tf_vars.vpc_id.value }}"
```

## 🔍 Testing & Validation

### 1. Terraform Testing
```bash
# Validate Terraform files
terraform validate

# Format Terraform files
terraform fmt -recursive

# Plan without applying
terraform plan

# Show current state
terraform show
```

### 2. Ansible Testing
```bash
# Syntax check
ansible-playbook --syntax-check playbooks/site.yml

# Dry run
ansible-playbook --check playbooks/site.yml

# Run with verbose output
ansible-playbook -vvv playbooks/site.yml

# Test specific hosts
ansible-playbook --limit webservers playbooks/site.yml
```

### 3. Infrastructure Testing
```bash
# Test connectivity
ansible all -m ping

# Check services
ansible webservers -m service -a "name=nginx" --become

# Verify application
curl -s http://$(cd terraform && terraform output -raw load_balancer_dns) | jq .
```

## 🚀 Usage Examples

### 1. Full Deployment
```bash
# Deploy everything
./scripts/deploy.sh deploy

# Or step by step
./scripts/deploy.sh infra-only
./scripts/deploy.sh config-only
./scripts/deploy.sh verify
```

### 2. Update Application Only
```bash
# Update application code
ansible-playbook -i ansible/inventory/hosts ansible/playbooks/site.yml --tags application
```

### 3. Scale Infrastructure
```bash
# Update terraform.tfvars
echo 'web_server_count = 4' >> terraform/terraform.tfvars

# Apply changes
cd terraform && terraform apply
cd ../ansible && ansible-playbook -i inventory/hosts playbooks/site.yml
```

### 4. Rolling Updates
```yaml
# In Ansible playbook
- name: Rolling update
  hosts: webservers
  serial: 1  # Update one server at a time
  tasks:
    - name: Update application
      # ... update tasks
```

## 🛡️ Security Best Practices

### 1. Terraform Security
```hcl
# Use specific AMI IDs (not latest)
# Restrict security group rules
# Enable encryption
# Use IAM roles instead of keys

resource "aws_instance" "web" {
  # ... other config ...
  
  # Use IAM instance profile
  iam_instance_profile = aws_iam_instance_profile.web.name
  
  # Encrypt EBS volumes
  root_block_device {
    encrypted = true
  }
}
```

### 2. Ansible Vault for Secrets
```bash
# Create vault file
ansible-vault create ansible/group_vars/all/vault.yml

# Add encrypted variables
db_password: !vault |
  $ANSIBLE_VAULT;1.1;AES256
  66386439653...
```

### 3. SSH Key Management
```bash
# Generate SSH key pair
ssh-keygen -t rsa -b 4096 -f ~/.ssh/terraform-key

# Add public key to AWS
aws ec2 import-key-pair --key-name terraform-key --public-key-material fileb://~/.ssh/terraform-key.pub
```

## 📊 Monitoring & Logging

### CloudWatch Integration
```hcl
# In Terraform
resource "aws_cloudwatch_log_group" "app" {
  name              = "/aws/ec2/${var.environment}"
  retention_in_days = 7
}
```

### Ansible Logging
```yaml
# In Ansible playbook
- name: Configure logging
  template:
    src: rsyslog.conf.j2
    dest: /etc/rsyslog.d/app.conf
  notify: restart rsyslog
```

## 🚀 Pro Tips for Busy Developers

1. **Use workspaces** for multiple environments:
   ```bash
   terraform workspace new staging
   terraform workspace select production
   ```

2. **Separate Terraform state** by environment
3. **Use Terraform modules** for reusable components
4. **Implement CI/CD pipeline** for automated deployment
5. **Use Ansible tags** for selective deployment
6. **Monitor costs** with AWS Cost Explorer
7. **Backup Terraform state** to S3 with versioning
8. **Use Terraform Cloud** for team collaboration

## 🔧 Troubleshooting Guide

### Common Issues

1. **SSH connection failed**
   ```bash
   # Check security groups
   # Verify SSH key
   ssh -i ~/.ssh/key.pem ubuntu@<ip>
   ```

2. **Terraform state lock**
   ```bash
   terraform force-unlock <lock-id>
   ```

3. **Ansible unreachable hosts**
   ```bash
   # Test SSH connectivity
   ansible all -m ping -vvv
   
   # Check inventory
   ansible-inventory --list
   ```

4. **Load balancer health checks failing**
   ```bash
   # Check target group health
   aws elbv2 describe-target-health --target-group-arn <arn>
   
   # Test application directly
   curl -I http://<instance-ip>:80/health
   ```

5. **Terraform apply fails**
   ```bash
   # Check AWS credentials
   aws sts get-caller-identity
   
   # Validate configuration
   terraform validate
   
   # Import existing resources if needed
   terraform import aws_instance.web i-1234567890abcdef0
   ```

## 🔄 CI/CD Integration

### GitHub Actions Workflow
```yaml
# .github/workflows/deploy.yml
name: Deploy Infrastructure and Applications

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  AWS_REGION: us-west-2
  TF_VERSION: 1.5.0
  ANSIBLE_VERSION: 6.0.0

jobs:
  terraform:
    name: Terraform
    runs-on: ubuntu-latest
    outputs:
      lb_dns: ${{ steps.terraform.outputs.lb_dns }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: ${{ env.TF_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Terraform Init
      working-directory: terraform
      run: terraform init

    - name: Terraform Plan
      working-directory: terraform
      run: terraform plan

    - name: Terraform Apply
      if: github.ref == 'refs/heads/main'
      working-directory: terraform
      run: terraform apply -auto-approve
      id: terraform

    - name: Output
      if: github.ref == 'refs/heads/main'
      working-directory: terraform
      run: |
        echo "lb_dns=$(terraform output -raw load_balancer_dns)" >> $GITHUB_OUTPUT

  ansible:
    name: Ansible
    runs-on: ubuntu-latest
    needs: terraform
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install Ansible
      run: |
        pip install ansible==${{ env.ANSIBLE_VERSION }}
        pip install boto3 botocore

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Add SSH key
      run: |
        mkdir -p ~/.ssh
        echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/terraform-key.pem
        chmod 600 ~/.ssh/terraform-key.pem

    - name: Wait for instances
      run: sleep 60

    - name: Run Ansible
      working-directory: ansible
      run: |
        ansible-playbook -i inventory/hosts playbooks/site.yml

    - name: Test deployment
      run: |
        curl -f http://${{ needs.terraform.outputs.lb_dns }}/health || exit 1

  cleanup:
    name: Cleanup (if PR)
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: ${{ env.TF_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Terraform Destroy
      working-directory: terraform
      run: |
        terraform init
        terraform destroy -auto-approve
```

### GitLab CI/CD Pipeline
```yaml
# .gitlab-ci.yml
stages:
  - validate
  - plan
  - deploy
  - configure
  - test
  - cleanup

variables:
  TF_ROOT: terraform
  ANSIBLE_ROOT: ansible
  AWS_DEFAULT_REGION: us-west-2

before_script:
  - apt-get update -qq && apt-get install -y -qq git curl unzip
  - curl -fsSL https://apt.releases.hashicorp.com/gpg | apt-key add -
  - apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"
  - apt-get update -qq && apt-get install -y -qq terraform
  - pip3 install ansible boto3

validate:
  stage: validate
  script:
    - cd $TF_ROOT
    - terraform fmt -check
    - terraform validate
    - cd ../$ANSIBLE_ROOT
    - ansible-playbook --syntax-check playbooks/site.yml

plan:
  stage: plan
  script:
    - cd $TF_ROOT
    - terraform init
    - terraform plan
  artifacts:
    name: plan
    paths:
      - $TF_ROOT/.terraform
    expire_in: 1 hour

deploy:
  stage: deploy
  script:
    - cd $TF_ROOT
    - terraform init
    - terraform apply -auto-approve
  artifacts:
    name: terraform-outputs
    paths:
      - $TF_ROOT/terraform.tfstate
    expire_in: 1 week
  only:
    - main

configure:
  stage: configure
  script:
    - mkdir -p ~/.ssh
    - echo "$SSH_PRIVATE_KEY" | tr -d '\r' > ~/.ssh/terraform-key.pem
    - chmod 600 ~/.ssh/terraform-key.pem
    - sleep 60
    - cd $ANSIBLE_ROOT
    - ansible-playbook -i inventory/hosts playbooks/site.yml
  dependencies:
    - deploy
  only:
    - main

test:
  stage: test
  script:
    - cd $TF_ROOT
    - LB_DNS=$(terraform output -raw load_balancer_dns)
    - curl -f http://$LB_DNS/health
  dependencies:
    - configure
  only:
    - main

cleanup:
  stage: cleanup
  script:
    - cd $TF_ROOT
    - terraform init
    - terraform destroy -auto-approve
  when: manual
  only:
    - main
```

## 🏗️ Advanced Terraform Patterns

### 1. Multi-Environment with Workspaces
```bash
# Create workspaces
terraform workspace new dev
terraform workspace new staging
terraform workspace new prod

# Use workspace-specific variables
# terraform/environments/dev.tfvars
environment = "dev"
instance_type = "t3.micro"
web_server_count = 1

# terraform/environments/prod.tfvars
environment = "prod"
instance_type = "t3.medium"
web_server_count = 3
```

### 2. Terraform Modules
```hcl
# modules/web-app/main.tf
variable "environment" {}
variable "instance_count" {}
variable "instance_type" {}

resource "aws_instance" "web" {
  count         = var.instance_count
  instance_type = var.instance_type
  # ... other configuration
}

output "instance_ips" {
  value = aws_instance.web[*].public_ip
}

# Use module in main.tf
module "web_app" {
  source = "./modules/web-app"
  
  environment    = var.environment
  instance_count = var.web_server_count
  instance_type  = var.instance_type
}
```

### 3. Remote State Backend
```hcl
# backend.tf
terraform {
  backend "s3" {
    bucket         = "my-terraform-state-bucket"
    key            = "terraform.tfstate"
    region         = "us-west-2"
    encrypt        = true
    dynamodb_table = "terraform-lock-table"
  }
}

# Create S3 bucket and DynamoDB table for state locking
resource "aws_s3_bucket" "terraform_state" {
  bucket = "my-terraform-state-bucket"
}

resource "aws_s3_bucket_versioning" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_dynamodb_table" "terraform_lock" {
  name           = "terraform-lock-table"
  billing_mode   = "PAY_PER_REQUEST"
  hash_key       = "LockID"

  attribute {
    name = "LockID"
    type = "S"
  }
}
```

## 🔄 Advanced Ansible Patterns

### 1. Ansible Tower/AWX Integration
```yaml
# awx_config.yml
- name: Configure AWX Job Template
  awx.awx.job_template:
    name: "Deploy Web Application"
    job_type: "run"
    inventory: "AWS Dynamic Inventory"
    project: "Infrastructure Playbooks"
    playbook: "playbooks/site.yml"
    credential: "AWS Credentials"
    state: present
```

### 2. Rolling Deployments with Load Balancer
```yaml
# playbooks/rolling-deploy.yml
- name: Rolling deployment
  hosts: webservers
  serial: "{{ (ansible_play_hosts | length * 0.3) | round(0, 'ceil') | int }}"
  max_fail_percentage: 25
  
  pre_tasks:
    - name: Remove from load balancer
      local_action:
        module: elb_target
        target_group_arn: "{{ target_group_arn }}"
        target_id: "{{ ansible_ec2_instance_id }}"
        state: absent
        target_status: draining
      
  tasks:
    - name: Deploy new version
      include_role:
        name: application
      vars:
        app_version: "{{ new_version }}"
        
  post_tasks:
    - name: Add back to load balancer
      local_action:
        module: elb_target
        target_group_arn: "{{ target_group_arn }}"
        target_id: "{{ ansible_ec2_instance_id }}"
        state: present
        
    - name: Wait for health check
      local_action:
        module: elb_target_info
        target_group_arn: "{{ target_group_arn }}"
      register: target_health
      until: target_health.targets[0].target_health.state == "healthy"
      retries: 30
      delay: 10
```

### 3. Blue-Green Deployment
```yaml
# playbooks/blue-green-deploy.yml
- name: Blue-Green Deployment
  hosts: localhost
  vars:
    current_color: "{{ 'blue' if active_environment == 'green' else 'green' }}"
    
  tasks:
    - name: Deploy to inactive environment
      include: deploy-environment.yml
      vars:
        environment_color: "{{ current_color }}"
        
    - name: Run health checks
      uri:
        url: "http://{{ hostvars[item]['ansible_host'] }}/health"
        method: GET
      loop: "{{ groups[current_color + '_servers'] }}"
      
    - name: Switch load balancer traffic
      elb_target_group:
        name: "web-{{ current_color }}-tg"
        protocol: http
        port: 80
        vpc_id: "{{ vpc_id }}"
        targets:
          - Id: "{{ hostvars[item]['ansible_ec2_instance_id'] }}"
            Port: 80
        state: present
      loop: "{{ groups[current_color + '_servers'] }}"
      
    - name: Update active environment marker
      set_fact:
        active_environment: "{{ current_color }}"
```

## 🔧 Monitoring and Observability

### 1. CloudWatch Integration
```yaml
# roles/monitoring/tasks/main.yml
- name: Install CloudWatch agent
  get_url:
    url: https://s3.amazonaws.com/amazoncloudwatch-agent/amazon_linux/amd64/latest/amazon-cloudwatch-agent.rpm
    dest: /tmp/amazon-cloudwatch-agent.rpm

- name: Install CloudWatch agent package
  yum:
    name: /tmp/amazon-cloudwatch-agent.rpm
    state: present

- name: Configure CloudWatch agent
  template:
    src: cloudwatch-config.json.j2
    dest: /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json
  notify: restart cloudwatch agent

- name: Start CloudWatch agent
  service:
    name: amazon-cloudwatch-agent
    state: started
    enabled: yes
```

### 2. Application Performance Monitoring
```yaml
# roles/apm/tasks/main.yml
- name: Install APM agent
  npm:
    name: "@newrelic/native-metrics"
    path: "/opt/{{ app_name }}"
    global: no

- name: Configure APM
  template:
    src: newrelic.js.j2
    dest: "/opt/{{ app_name }}/newrelic.js"
  notify: restart application

- name: Update application to use APM
  lineinfile:
    path: "/opt/{{ app_name }}/app.js"
    line: "require('newrelic');"
    insertbefore: BOF
  notify: restart application
```

### 3. Log Aggregation
```yaml
# roles/logging/tasks/main.yml
- name: Install Filebeat
  get_url:
    url: https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.15.0-amd64.deb
    dest: /tmp/filebeat.deb

- name: Install Filebeat package
  apt:
    deb: /tmp/filebeat.deb

- name: Configure Filebeat
  template:
    src: filebeat.yml.j2
    dest: /etc/filebeat/filebeat.yml
  notify: restart filebeat

- name: Start Filebeat
  service:
    name: filebeat
    state: started
    enabled: yes
```

## 🛡️ Security Hardening

### 1. SSL/TLS Configuration
```yaml
# roles/ssl/tasks/main.yml
- name: Install Certbot
  apt:
    name: 
      - certbot
      - python3-certbot-nginx
    state: present

- name: Obtain SSL certificate
  command: >
    certbot --nginx 
    --non-interactive 
    --agree-tos 
    --email {{ ssl_email }}
    --domains {{ domain_name }}
  args:
    creates: "/etc/letsencrypt/live/{{ domain_name }}/fullchain.pem"

- name: Configure SSL renewal cron
  cron:
    name: "Renew SSL certificates"
    minute: "0"
    hour: "2"
    job: "certbot renew --quiet"
```

### 2. Security Groups Automation
```hcl
# security.tf
resource "aws_security_group_rule" "allow_ssh_from_specific_ips" {
  count             = length(var.admin_ips)
  type              = "ingress"
  from_port         = 22
  to_port           = 22
  protocol          = "tcp"
  cidr_blocks       = [var.admin_ips[count.index]]
  security_group_id = aws_security_group.web.id
}

# Use with Ansible vault for IP management
variable "admin_ips" {
  description = "List of admin IP addresses"
  type        = list(string)
  default     = []
}
```

### 3. Secrets Management
```yaml
# playbooks/secrets-management.yml
- name: Retrieve secrets from AWS Secrets Manager
  aws_secret:
    name: "{{ environment }}/database"
    region: "{{ aws_region }}"
  register: db_secrets

- name: Set database credentials
  set_fact:
    db_password: "{{ db_secrets.secret.password }}"
    db_username: "{{ db_secrets.secret.username }}"
  no_log: true
```

## 📈 Cost Optimization

### 1. Instance Right-Sizing
```hcl
# cost-optimization.tf
resource "aws_instance" "web" {
  # Use data source to get optimal instance type
  instance_type = data.aws_ec2_instance_type_offering.optimal.instance_type
  
  # Enable detailed monitoring for cost analysis
  monitoring = true
  
  # Use spot instances for non-production
  instance_market_options {
    market_type = var.environment == "prod" ? null : "spot"
    spot_options {
      instance_interruption_behavior = "terminate"
      max_price                      = "0.05"
    }
  }
}
```

### 2. Auto Scaling Integration
```hcl
# autoscaling.tf
resource "aws_launch_template" "web" {
  name_prefix   = "${var.environment}-web-"
  image_id      = data.aws_ami.ubuntu.id
  instance_type = var.instance_type
  key_name      = var.key_name

  vpc_security_group_ids = [aws_security_group.web.id]

  user_data = base64encode(templatefile("${path.module}/user_data.sh", {
    ansible_playbook_url = "https://github.com/your-repo/ansible-playbooks.git"
  }))

  tag_specifications {
    resource_type = "instance"
    tags = {
      Name        = "${var.environment}-web-asg"
      Environment = var.environment
    }
  }
}

resource "aws_autoscaling_group" "web" {
  name                = "${var.environment}-web-asg"
  vpc_zone_identifier = aws_subnet.public[*].id
  target_group_arns   = [aws_lb_target_group.web.arn]
  health_check_type   = "ELB"
  min_size            = var.min_servers
  max_size            = var.max_servers
  desired_capacity    = var.desired_servers

  launch_template {
    id      = aws_launch_template.web.id
    version = "$Latest"
  }

  tag {
    key                 = "Name"
    value               = "${var.environment}-web-asg"
    propagate_at_launch = true
  }
}
```

## 🎯 Best Practices Summary

### Terraform Best Practices
1. **Use remote state** with S3 and DynamoDB locking
2. **Implement workspaces** for multiple environments
3. **Create reusable modules** for common patterns
4. **Use data sources** instead of hardcoded values
5. **Tag all resources** consistently
6. **Validate configurations** before applying
7. **Use specific provider versions** to avoid breaking changes

### Ansible Best Practices
1. **Use roles** for modularity and reusability
2. **Implement idempotency** in all tasks
3. **Use handlers** for service restarts
4. **Leverage tags** for selective execution
5. **Encrypt sensitive data** with Ansible Vault
6. **Test playbooks** in staging environments
7. **Use dynamic inventories** for cloud environments

### Integration Best Practices
1. **Separate concerns**: Terraform for infrastructure, Ansible for configuration
2. **Use consistent naming** across both tools
3. **Implement proper error handling** and rollback strategies
4. **Monitor and log** all operations
5. **Automate testing** with CI/CD pipelines
6. **Document dependencies** and prerequisites
7. **Plan for disaster recovery** and backup strategies

## 🚀 Quick Start Commands

```bash
# Clone and setup
git clone <your-repo>
cd terraform-ansible-project

# Full deployment
./scripts/deploy.sh deploy

# Update only configuration
./scripts/deploy.sh config-only

# Verify deployment
./scripts/deploy.sh verify

# Cleanup everything
./scripts/deploy.sh destroy
```

---

**🎉 You now have a complete Terraform + Ansible integration setup!**

This guide provides everything you need to:
- Provision AWS infrastructure with Terraform
- Configure servers with Ansible
- Deploy applications automatically
- Implement CI/CD pipelines
- Monitor and maintain your infrastructure

Start with the basic setup and gradually implement advanced patterns as your needs grow. The modular approach allows you to adapt this framework to any cloud provider or application stack.
